{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse and comparision of recomender systems algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I analysed 3 different algorithms for recomender systems. Firstly, most basic algorithm - user user colaborative filtering, with cosine similarity measure and weighted average. Second was Funk singular value decomposition algorithm, presented at netflix prize in 2009. And third one was slope one, with weighted average. I checked how well all three of them are doing based on time needed and mean square error. Lastly I picked the best one and analysed it somehow deeper, with custom made measure of wellness. I also tried to somehow analyse time complexity of this algorithms, keeping in mind that numpy and numba magic could and probably would interupt results, but general relation of time given variables should been kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User user colaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User user collaborative filtering algorithm idea is to take movie which user did not rated, find similiar users to him who rated this movie (how much of this similiar users we will take is hyperparameter), take their weighted average of ratings of this movie (weight is cosine distance beetwen our user and them) and this is our rating of movie for user.\n",
    "Beacuse we are doing for every cell which rating we want to predict, time complexity will be:\n",
    "- number of test cases\n",
    "- log(movies) beacuse I mapped each movie to users who rated it\n",
    "- O(1) I treat cosine distance as one operation (numpy magic)\n",
    "- k * n or nlogn beacuse we need to find minimum k times and every user could rated this movie, or just sort users by their cosine distance (in my implementation it is k * n)\n",
    "- k beacuse we compute weighted average for k nearest users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this gives us O(number_of_test_cases * log(movies) * k * n * k), worst case scenario, realistically n should around 1/100 * n beacuse in 10m dataset we have density around 1%.\n",
    "Lowest MSE which I got was at level 1.06 which is around 2.5 times better than random recomendation, and time to compute just first 200 test cases was 140 seconds on 10m dataset and 11seconds to compute all 20k test cases on 100k data set. 140 seconds for 1/10000 of test set is little bit too slow for algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funk svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funk svd basic idea is to decompose matrix M of size n x m to two matrixes P (size n x k) and Q (m x k) which will correspond to users and items. We also try to include some user/movie specific bias and global bias to our predictions. So our prediction of rating will be given by:\n",
    "#### r[i][j] =  P[i].dot(Q[j]) + user_bias[i] + item_bias[j] + global_bias\n",
    "And we will try to estimate P,Q,user_bias and item_bias so we minimize our loss function:\n",
    "#### sum_over_i,j (M[i][j] - P[i].dot(Q[j]) + user_bias[i] +item_bias[j] +global_bias)^2 + reg*(||P||^2+||Q||^2+user_bias^2+item_bias^2)\n",
    "Little bit lower I wrote how to update each vector/matrix in every iteration of algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time complexity of this algorithm is:\n",
    "- number of iterations\n",
    "- number of known cells (we are doing update for each cell seperately)\n",
    "- k (we are doing for loop for i in range(k))\n",
    "\n",
    "And prediciting after that is just O(number_of_test_cases). It is really fast algorithm (500 seconds on 10m dataset), which gives 0.63 MSE (nearly 2 times better than user user) without hardcore tuning of hyperparameters. Only downside of this algorithm are three hyperparameters to tune in comparision to one in user user colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slopne one basic idea is to take movie which is not rated by user, and rate it by weighted average of differences beetwen this movie and another movies which another users rated (and our user) where weight is number of users who rated both movies. So for example if:\n",
    "- user A rated movie \"Toy story\" with 4, \"Monty Python\" with 2 and we want to know how he will rate \"Pokemon\" \n",
    "- we take user B who rated \"Toy story\" with 3 and \"Pokemon\" with 4 and \"Monty Python\" with 5\n",
    "- and user C who rated \"Toy Story\" with 4 and \"Pokemon\" with 1\n",
    "- \"Pokemons\" are generally rated 1 mark lower than \"Toy Story\" -> ((4-3) + (1-4))/2 = -1\n",
    "- \"Pokemons\" are generally rated 1 mark lower than \"Monty Python\" -> (4-5)/1 = -1\n",
    "- so user A will probably rate \"Pokemon\" with rating ((4-1) * 2 + (2-1) * 1)/3 = 2.333\n",
    "\n",
    "So firstly we construct matrix of size movies x movies, compute differences beetwen ratings of movies, and then we predict ratings by using this matrix. Time complexity of it will be:\n",
    "- number of movies * number_of_movies * number_of_users (constructing matrix of differences)\n",
    "- number of test cases * number_of_users (for each test case we need to look for all movies which user rated)\n",
    "\n",
    "So mainly first part of algorithm is slow (20 hours on 10m dataset but using cython and some tricks suprise package achives this same thing in something beetwen 1000-3000 seconds, more about surprise later), but it gets 0.73 MSE which is somehow good score. And one thing which is also worth noticing is that there are no hyperparameters to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision of my implementation with surprise (package of recomender systems algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funk svd:\n",
    "- mse:\n",
    "    - my 0.6360918021837733\n",
    "    - surprise 0.6366\n",
    "- time:\n",
    "    - my 547s\n",
    "    - surprise 468s\n",
    "\n",
    "#### Slope One\n",
    "- mse:\n",
    "    - my 0.8879477416535028\n",
    "    - surprise 0.7407\n",
    "- time:\n",
    "    - my 20h\n",
    "    - surprise 1000s-3000s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about slope one, I checked implementation of it in surprise package, and algorithm is really similiar, so speed up is probably achived by the fact that they are using cython."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Somehow deeper analyse of my funk svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So mse is nice, it is easy and fast to compute, but it shows how well we are doing in predicting ratings after hiding some of them, not necessarily how well we recomend movies. So I constructed another measure function which will check how much user likes each movie category (another thing is that this is interesting info which probably can be used to another approach of recomender systems) then I will recomend to user some movies using svd and see how much of these recomendations have higher \"category liking\" score than movies which user already saw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So firstly we map each movie to categories, then we construct matrix C of size number_of_movies x number_of_categories with 1 in cell i,j if movie i is from category j and 0 otherwise. Then we create matrix U of size number_of_users x number_of_categories by computing F for every user and every movie that this user liked.\n",
    "Function F is defined as follows:\n",
    "- bias is average of ratings given by user\n",
    "- if user u rated movie c higher or equal 3 then multiplier for movie is 1+(movie_rating-bias)\n",
    "- if user u rated movie c lower than 3 then multiplier for movie is -(1-(movie_rating-bias))\n",
    "- U[u] += C[c] * multiplier\n",
    "\n",
    "At the end of this we have matrix U filled with estimated likings of categories by each user. Now for each movie that user watched we can compute \"category score\" for this movie which is simply U[u].dot(C[c]) divided by number of categories that movie represents. We take average of these \"category scores\" for each user and then we recommend 10 movies for each user, compute their \"category scores\" and check if their score is higher than mean.\n",
    "Funk svd gets 69% with this measure, which i think is pretty great - 69% movies which we are recommending should be interesting for user, well in terms of movies categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future predicting score of svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiding random cells of matrix M is okey idea for testing algorithm but better approach would be hiding x% of newest ratings, and then checking MSE. After doing it, funk svd MSE is getting worse, from 0.63 to 1.17, what sugest that is harder to predict future than predicting random cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before running code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should be in the same folder as movielens 100k and 10m folders.\n",
    "I use pd.read_csv(\"ml-100k/u.data\"),pd.read_csv(\"ml-10M100K/ratings.dat) and pd.read(\"ml-10M100K/movies.dat\") so paths to this files from jupyter notebook should be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "import matplotlib.pyplot as plt\n",
    "import surprise\n",
    "from numba.typed import Dict\n",
    "from numba import types\n",
    "import numba\n",
    "from numba import jit\n",
    "from time import time\n",
    "from numba import prange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So I will use 100k dataset for general fast experimentation and 10m for real comparision of algorithms\n",
    "data_100k = pd.read_csv(\"ml-100k/u.data\",header = None,names = ['user','movie','rating','timestamp'],sep = \"\\t\")\n",
    "data_10m = pd.read_csv(\"ml-10M100K/ratings.dat\",header = None,names = [\"user\",\"movie\",\"rating\",\"timestamp\"],sep = \"::\",encoding = \"utf8\",engine = \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_100k = data_100k.drop('timestamp',axis=1)\n",
    "data_10m = data_10m.drop('timestamp',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_100k = data_100k.to_numpy()\n",
    "data_10m = data_10m.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#so in 10m dataset movies ids are from range 1 to 65133 but we have data about only 10677 of them so we need to map\n",
    "#their id, (in dataset there is for example movie id 4001 and next one has id 4021 when they should be x and x+1)\n",
    "@njit\n",
    "def hash_movies(data):\n",
    "    table = dict()\n",
    "    reversed_table = dict()\n",
    "    cnt = 0\n",
    "    for ite in range(data.shape[0]):\n",
    "        if not data[ite][1] in table:\n",
    "            table[data[ite][1]] = cnt\n",
    "            reversed_table[cnt] = data[ite][1]\n",
    "            cnt+=1\n",
    "        data[ite][1] = table[data[ite][1]]\n",
    "    return (table,reversed_table)\n",
    "\n",
    "tables_10m = hash_movies(data_10m)\n",
    "data_100k[:,1] = data_100k[:,1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data_100k = np.random.permutation(data_100k)\n",
    "train_100k,test_100k = np.split(shuffled_data_100k,[int(0.8*data_100k.shape[0])])\n",
    "test_100k[:,0] = test_100k[:,0] - 1\n",
    "shuffled_data_10m = np.random.permutation(data_10m)\n",
    "train_10m,test_10m = np.split(shuffled_data_10m,[int(0.8*data_10m.shape[0])])\n",
    "test_10m[:,0] = test_10m[:,0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def construct_Matrix(data,users_num,items_num):\n",
    "    matrix = np.ones((users_num,items_num))\n",
    "    matrix[:] = np.nan\n",
    "    for row in range(data.shape[0]):\n",
    "        matrix[int(data[row][0]-1),int(data[row][1])] = data[row][2]\n",
    "    return matrix\n",
    "\n",
    "train_100k_mat = construct_Matrix(train_100k,943,1682)\n",
    "train_10m_mat = construct_Matrix(train_10m,71567,10681)\n",
    "data_100k_mat = construct_Matrix(data_100k,943,1682)\n",
    "data_10m_mat = construct_Matrix(data_10m,71567,10681)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#well making it function speed it up more than 4 times (average time for on 10m without njit was 30sec and 7sec with njit)\n",
    "@njit\n",
    "def global_mean(original_matrix):\n",
    "    return np.nanmean(original_matrix)\n",
    "\n",
    "global_bias_100k = global_mean(train_100k_mat)\n",
    "global_bias_10m = global_mean(train_10m_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random recomendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 100k:  2.876282853876754\n",
      "MSE 10m:  2.7207939092634303\n"
     ]
    }
   ],
   "source": [
    "#just to have some reference\n",
    "m_100k = 0\n",
    "m_10m = 0\n",
    "for i in range(100):\n",
    "    rng_100k = np.random.uniform(1,5,test_100k.shape[0])\n",
    "    rng_10m = np.random.uniform(1,5,test_10m.shape[0])\n",
    "    m_100k += np.mean((test_100k[:,2]-rng_100k)**2)\n",
    "    m_10m += np.mean((test_10m[:,2]-rng_10m)**2)\n",
    "print(\"MSE 100k: \",m_100k/100)\n",
    "print(\"MSE 10m: \",m_10m/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-user collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic idea of user-user collaborative filtering is:\n",
    "1. we want to rate movie y for user x\n",
    "2. so we take every user who rated movie y and measure their similarity to user x\n",
    "3. after that we take k most similar to user x users and compute their weighted average of ratings of movie y\n",
    "4. this is our predicted rating of user x on movie y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#so here we are remembering for each item which user rated it, it will speed up filtering\n",
    "@njit\n",
    "def remember_users(original_matrix,rem_user):\n",
    "    for y in range(original_matrix.shape[1]):\n",
    "        ar = []\n",
    "        for x in range(original_matrix.shape[0]):\n",
    "            if not np.isnan(original_matrix[x][y]):\n",
    "                ar.append(x)\n",
    "        arr = np.array(ar)\n",
    "        if arr.size != 0:\n",
    "            rem_user[y] = arr\n",
    "\n",
    "rem_user_100k = Dict.empty(key_type = types.int64,value_type = types.int64[:])\n",
    "rem_user_10m = Dict.empty(key_type = types.int64,value_type = types.int64[:])\n",
    "remember_users(train_100k_mat,rem_user_100k)\n",
    "remember_users(train_10m_mat,rem_user_10m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to copy original_matrix and fill copied matrix rows with means for each row (means for user)\n",
    "@njit\n",
    "def mean_user(original_matrix,replacer = 0):\n",
    "    new_matrix = np.copy(original_matrix)\n",
    "    for row in range(new_matrix.shape[0]):\n",
    "        bias = np.nanmean(new_matrix[row,:])\n",
    "        if np.isnan(bias):\n",
    "            bias = replacer\n",
    "        new_matrix[row][np.isnan(new_matrix[row,:])] = bias\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_100k_mat_user = mean_user(train_100k_mat,global_bias_100k)\n",
    "train_10m_mat_user = mean_user(train_10m_mat,global_bias_10m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#well there was a choice, no for just smart numpy matrix operations or njit and for, after some tests I choosed njit\n",
    "#mainly beacuse njit funcs can only call njit funcs so after running smart numpy vs not so smart numpy with njit\n",
    "#the champion was clearly njit\n",
    "@njit\n",
    "def cosinus_ratio(me,neighbour):\n",
    "    arr = []\n",
    "    for i in range(neighbour.shape[0]):\n",
    "        arr.append(np.dot(me,neighbour[i,:])/(np.linalg.norm(me)*np.linalg.norm(neighbour[i,:])))\n",
    "    return np.array(arr)\n",
    "\n",
    "@njit\n",
    "def get_std_user(original_matrix,neighbours,user,y_axis,k,weighted = True):\n",
    "    similarities = cosinus_ratio(user,neighbours)\n",
    "    #indices = np.argsort(similarities)\n",
    "    #indices = indices[:k]\n",
    "    simi = np.copy(similarities)\n",
    "    indi = []\n",
    "    k = min(k,simi.size)\n",
    "    for i in range(k):\n",
    "        smallest = np.argmax(simi)\n",
    "        indi.append(smallest)\n",
    "        simi[smallest] = -100\n",
    "    indices = np.array(indi)\n",
    "    mean = 0\n",
    "    weights = 0\n",
    "    for neighbour in indices:\n",
    "        if weighted:\n",
    "            mean += neighbours[neighbour,y_axis]*similarities[neighbour]\n",
    "            weights += similarities[neighbour]\n",
    "        else:\n",
    "            mean+= neighbours[neighbour,y_axis]\n",
    "    if weighted:\n",
    "        return mean/weights\n",
    "    else:\n",
    "        return mean/k\n",
    "    \n",
    "@njit\n",
    "def user_user_colab(original_matrix,test,rem_user,k,weighted=True,replacer = 0):\n",
    "    ratings = []\n",
    "    for i in range(test.shape[0]):\n",
    "        user_x = test[i][0]\n",
    "        item_y = test[i][1]\n",
    "        if not item_y in rem_user:\n",
    "            ratings.append(replacer)\n",
    "        else:\n",
    "            res = get_std_user(original_matrix,original_matrix[rem_user[item_y],:],original_matrix[user_x,:],item_y,k,weighted)\n",
    "            ratings.append(res)\n",
    "    return np.array(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.437665491104125\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#okay so few things about user-user-colab, it takes 12 seconds for 20k questions on roughly 1000x1600 matrix\n",
    "#that is 0.0006s for one question\n",
    "#let n be number of users and m number of movies\n",
    "#for every question it needs to ask dictionary (log(m) complexity) then compute cosine for each user who rated\n",
    "#that movie - O(n) (if we say that cosinus_ratio complexity is O(1)),then finding k biggest is k*n for argmax\n",
    "#or nlogn for argsort so in total we have single question complexity is something like log(m)*n*log(n)*n\n",
    "#we have sparse data so we can divide it by 100 or even more probably (there wont be a lot of users who all\n",
    "#rated some movie), but even though it is still (n^2)*logn*logm for question - really slow\n",
    "#yes of course there is magic numpy matrix stuff but I treated matrix calculations as O(1) complexity\n",
    "#even if we change asking each user who rated movie complexity from O(n) to O(1) it will still be n*logn*logm\n",
    "#but it is still time for single question!\n",
    "#results are not really bad MSE at level 1.3 is not best but also not worst result we could have\n",
    "#better than random that is for sure (but also much much slower)\n",
    "test_100k_user = test_100k[:,:2].astype(int)\n",
    "MSE = []\n",
    "av_time = 0\n",
    "for k in range(1,101):\n",
    "    a = time()\n",
    "    result_100k = user_user_colab(train_100k_mat_user,test_100k_user,rem_user_100k,k,True,global_bias_100k)\n",
    "    av_time += time() - a\n",
    "    MSE.append(np.mean((test_100k[:,2]-result_100k)**2))\n",
    "print(av_time/100)\n",
    "plt.plot(np.arange(1,101,1),MSE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest MSE:  1.0604523846286376\n"
     ]
    }
   ],
   "source": [
    "print(\"Lowest MSE: \",np.min(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.53783869743347\n",
      "MSE:  1.078460268100709\n",
      "MAE:  0.7962495546980602\n"
     ]
    }
   ],
   "source": [
    "#so lets see how horrible it gets at 10m movie lens dataset, it takes 200 seconds for 200 question on 70000x10000 \n",
    "#matrix, it is 1 second for one question!\n",
    "#70000*log2(70000)*log2(10000)/1000*log2(1000)*log2(1600) = 140\n",
    "#0.0006*140=0.084 so time complexity of user-user is even bigger than n*logn*logm\n",
    "#2m questions with speed 1question/1s is 23,14 days.\n",
    "#So conclusion is that user-user colab is pretty bad approach, even though it gets somehow fine MSE score, it is worth\n",
    "#noticing that MSE score got better with data increase\n",
    "test_10m_user = test_10m[:,:2].astype(int)\n",
    "test_10m_user_200sample = test_10m_user[:200,:]\n",
    "a = time()\n",
    "result_10m_200sample = user_user_colab(train_10m_mat_user,test_10m_user_200sample,rem_user_10m,10,True,global_bias_10m)\n",
    "print(time()-a)\n",
    "print(\"MSE: \",np.mean((test_10m[:200,2]-result_10m_200sample)**2))\n",
    "print(\"MAE: \",np.mean(np.abs(test_10m[:200,2]-result_10m_200sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funk SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic idea of funk singular value decomposition is:\n",
    "1. We want to express our ratings matrix M of shape n x m, as P.dot(Q) where P is matrix of shape n x k and each of n rows coresponds to some numbers describing user n, and Q is analogically movies matrix with this same properties as P.\n",
    "2. Another thing which we want is computing P and Q in a way that M-P.dot(Q) is minimal on known cells.\n",
    "3. We also introduce global bias and biases for each user and item to somehow solve problem of users rating everything high and users rating everything low\n",
    "4. Then P.dot(Q) will give us some approximation of matrix M (now we have each cell filled) and hopefully good one.\n",
    "5. More formally:\n",
    "    1. We have:\n",
    "        - gb = np.nanmean(M)\n",
    "        - bu = np.zeros(n)\n",
    "        - bi = np.zeros(m)\n",
    "        - P = np.random.normal(n,k)\n",
    "        - Q = np.random.normal(m,k)\n",
    "        - E[u][i] = gb+bu[u]+bi[i]+ for f in range(k): P[u][f]*Q[i][f]\n",
    "    2. The learning step is for each known rating update biases,P and Q as follows:\n",
    "        - ERR[u][i] = M[u][i]-E[u][i]\n",
    "        - Bu[u] = Bu[u] + lr*(ERR[u][i] - reg * Bu[u])\n",
    "        - Bi[i] = Bi[i] + lr*(ERR[u][i] - reg * Bi[u])\n",
    "        - P[u][f] = P[u][f] + lr * (ERR[u][i] * Q[i][f] - reg * P[u][f])\n",
    "        - Q[i][f] = Q[i][f] + lr * (ERR[u][i] * P[u][f] - reg * Q[i][f])\n",
    "        - lr is learning rate and reg is regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def matrix_factorize_funk(iterations,original_matrix,non_nan,global_bias,user_bias,item_bias,learning_rate,regularization,size,users,items):\n",
    "    for cnt in range(iterations):\n",
    "        non_nan = np.random.permutation(non_nan)\n",
    "        count = 0\n",
    "        for non in range(non_nan.shape[0]):\n",
    "            non0 = non_nan[non][0]\n",
    "            non1 = non_nan[non][1]\n",
    "            if np.isnan(original_matrix[non0][non1]):\n",
    "                print(\"ERROR\")\n",
    "                return None\n",
    "            estimate = global_bias + user_bias[non0] + item_bias[non1]\n",
    "            for lat in range(size):\n",
    "                estimate += users[non0,lat] * items[non1,lat]\n",
    "            err = original_matrix[non0,non1] - estimate\n",
    "            user_bias[non0] += learning_rate * (err - regularization * user_bias[non0])\n",
    "            item_bias[non1] += learning_rate * (err - regularization * item_bias[non1])\n",
    "            for lat in range(size):\n",
    "                usf = users[non0,lat]\n",
    "                itf = items[non1,lat]\n",
    "                users[non0,lat] += learning_rate * (err * itf - regularization * usf)\n",
    "                items[non1,lat] += learning_rate * (err * usf - regularization * itf)\n",
    "            count+=1\n",
    "    return (users,items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497.337769985199\n"
     ]
    }
   ],
   "source": [
    "#so funk svd is doing great, at least compared to user user collaborative filtering, total speed of 562 second for \n",
    "#anwsering 2M questions, well 512 seconds for being able to anwser any question about matrix, MSE is two times better\n",
    "#than user user colab, generally much better than user user. So first of all speed increase is due to fact that \n",
    "#complexity of algorithm is linearly proportional to number of known cells - which is much smaller than user*movies,\n",
    "#even smaller than user*log(movies). Second thing is that we include some global,user and movie biases to somehow\n",
    "#include generall tendency of rating high or low for each user, and generall tendency of being rated high if movie\n",
    "#is great,and rated low if movie is bad no matter of users preferences. Well only downside of funk svd is that\n",
    "#we need to set number of iterations,learning rate,regularization rate and size of matrixes, four hyperparameters to\n",
    "#choose and tune is much more than just one in user user colab\n",
    "bias_user = np.zeros(train_10m_mat.shape[0])\n",
    "bias_item = np.zeros(train_10m_mat.shape[1])\n",
    "non_nan = train_10m[:,:2]\n",
    "non_nan[:,0] -= 1\n",
    "non_nan = non_nan.astype(int)\n",
    "size = 15\n",
    "users = np.random.normal(scale = 0.1,size = [train_10m_mat.shape[0],size])\n",
    "items = np.random.normal(scale = 0.1,size = [train_10m_mat.shape[1],size])\n",
    "a = time()\n",
    "factorized_funk_10m = matrix_factorize_funk(100,train_10m_mat,non_nan,global_bias_10m,bias_user,bias_item,0.001,0.005,size,users,items)\n",
    "print(time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.31415295600891\n",
      "MSE:  0.6360918021837733\n"
     ]
    }
   ],
   "source": [
    "a = time()\n",
    "res_10m = np.dot(factorized_funk_10m[0],factorized_funk_10m[1].T)\n",
    "tes = test_10m[:,:2].astype(int)\n",
    "result_10m = []\n",
    "for user_x,item_y in tes:\n",
    "    if np.isnan(res_10m[user_x,item_y]):\n",
    "        print(user_x,item_y)\n",
    "    result_10m.append(global_bias_10m+bias_user[user_x]+bias_item[item_y]+res_10m[user_x,item_y])\n",
    "result_10m = np.array(result_10m)\n",
    "print(time()-a)\n",
    "print(\"MSE: \",np.mean((test_10m[:,2]-result_10m)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slope one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.059453032104637336\n",
      "0.11890606420927467\n",
      "0.7728894173602854\n",
      "0.535077288941736\n",
      "0.2972651605231867\n",
      "0.178359096313912\n",
      "0.356718192627824\n",
      "0.5945303210463734\n",
      "0.23781212841854935\n",
      "0.8323424494649228\n",
      "0.4161712247324614\n",
      "0.6539833531510107\n",
      "0.89179548156956\n",
      "0.4756242568370987\n",
      "0.713436385255648\n",
      "0.9512485136741974\n",
      "3.2142810821533203\n"
     ]
    }
   ],
   "source": [
    "#dev_mat[i][j] is difference of ratings movie[i] - movie[j] so if i want to get rating for movie y i need\n",
    "#sum over dev_mat[y][x] for every x\n",
    "@jit(nopython=True,parallel=True)\n",
    "def get_dev(original_matrix,dev_mat,dev_mat_user):\n",
    "    for i in prange(original_matrix.shape[1]):\n",
    "        if i%100==0:\n",
    "            print(i/original_matrix.shape[1])\n",
    "        for j in prange(original_matrix.shape[1]):\n",
    "            if j==i:\n",
    "                break\n",
    "            for u in range(original_matrix.shape[0]):\n",
    "                if not (np.isnan(original_matrix[u][i]) or np.isnan(original_matrix[u][j])):\n",
    "                    dev_mat[i][j] += original_matrix[u][i] - original_matrix[u][j]\n",
    "                    dev_mat[j][i] -= original_matrix[u][i] - original_matrix[u][j]\n",
    "                    dev_mat_user[i][j] += 1\n",
    "                    dev_mat_user[j][i] += 1\n",
    "\n",
    "dev_mat_100k = np.zeros(shape=[train_100k_mat.shape[1],train_100k_mat.shape[1]])\n",
    "dev_mat_100k_user = np.zeros(shape=[train_100k_mat.shape[1],train_100k_mat.shape[1]])\n",
    "a = time()\n",
    "get_dev(train_100k_mat,dev_mat_100k,dev_mat_100k_user)\n",
    "print(time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def predict_slope_one(original_matrix,test,dev_mat,dev_mat_user,replacer):\n",
    "    res = []\n",
    "    for i in range(test.shape[0]):\n",
    "        x = test[i][0]\n",
    "        y = test[i][1]\n",
    "        score = 0\n",
    "        weights = 0\n",
    "        for j in range(original_matrix.shape[1]):\n",
    "            if (not np.isnan(original_matrix[x][j])) and j!=y:\n",
    "                score += original_matrix[x][j]*dev_mat_user[y][j] + dev_mat[y][j]\n",
    "                weights += dev_mat_user[y][j]\n",
    "        if weights != 0:  \n",
    "            res.append(score/weights)\n",
    "        else:\n",
    "            res.append(replacer)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5318710803985596\n",
      "MSE:  0.8879477416535028\n"
     ]
    }
   ],
   "source": [
    "a = time()\n",
    "res_slope_one_100k = predict_slope_one(train_100k_mat,test_100k[:,:2].astype(int),dev_mat_100k,dev_mat_100k_user,global_bias_100k)\n",
    "print(time()-a)\n",
    "print(\"MSE: \",np.mean((test_100k[:,2]-res_slope_one_100k)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.009362419249133976\n",
      "0.018724838498267952\n",
      "0.02808725774740193\n",
      "0.25278531972661733\n",
      "0.037449676996535904\n",
      "0.04681209624566988\n",
      "0.05617451549480386\n",
      "0.06553693474393783\n",
      "0.5055706394532347\n",
      "0.07489935399307181\n",
      "0.26214773897575133\n",
      "0.08426177324220578\n",
      "0.09362419249133976\n",
      "0.10298661174047374\n",
      "0.27151015822488533\n",
      "0.7583559591798521\n",
      "0.11234903098960772\n",
      "0.12171145023874169\n",
      "0.5149330587023687\n",
      "0.2808725774740193\n",
      "0.13107386948787567\n",
      "0.14043628873700964\n",
      "0.2902349967231533\n",
      "0.14979870798614361\n",
      "0.5242954779515027\n",
      "0.1591611272352776\n",
      "0.767718378428986\n",
      "0.29959741597228723\n",
      "0.16852354648441156\n",
      "0.17788596573354554\n",
      "0.30895983522142123\n",
      "0.5336578972006366\n",
      "0.1872483849826795\n",
      "0.3183222544705552\n",
      "0.1966108042318135\n",
      "0.77708079767812\n",
      "0.2059732234809475\n",
      "0.5430203164497707\n",
      "0.3276846737196892\n",
      "0.21533564273008146\n",
      "0.3370470929688231\n",
      "0.22469806197921544\n",
      "0.5523827356989046\n",
      "0.2340604812283494\n",
      "0.786443216927254\n",
      "0.3464095122179571\n",
      "0.24342290047748338\n",
      "0.3557719314670911\n",
      "0.5617451549480386\n",
      "0.3651343507162251\n",
      "0.795805636176388\n",
      "0.5711075741971725\n",
      "0.374496769965359\n",
      "0.383859189214493\n",
      "0.805168055425522\n",
      "0.5804699934463066\n",
      "0.393221608463627\n",
      "0.5898324126954405\n",
      "0.402584027712761\n",
      "0.8145304746746559\n",
      "0.411946446961895\n",
      "0.5991948319445745\n",
      "0.4213088662110289\n",
      "0.82389289392379\n",
      "0.6085572511937084\n",
      "0.4306712854601629\n",
      "0.6179196704428425\n",
      "0.44003370470929687\n",
      "0.8332553131729239\n",
      "0.44939612395843087\n",
      "0.6272820896919764\n",
      "0.8426177324220578\n",
      "0.4587585432075648\n",
      "0.6366445089411104\n",
      "0.4681209624566988\n",
      "0.8519801516711918\n",
      "0.47748338170583277\n",
      "0.6460069281902444\n",
      "0.48684580095496677\n",
      "0.6553693474393784\n",
      "0.8613425709203258\n",
      "0.4962082202041007\n",
      "0.6647317666885123\n",
      "0.8707049901694598\n",
      "0.6740941859376463\n",
      "0.8800674094185937\n",
      "0.6834566051867803\n",
      "0.8894298286677278\n",
      "0.6928190244359143\n",
      "0.7021814436850482\n",
      "0.8987922479168617\n",
      "0.7115438629341821\n",
      "0.9081546671659957\n",
      "0.7209062821833162\n",
      "0.9175170864151296\n",
      "0.7302687014324502\n",
      "0.7396311206815841\n",
      "0.9268795056642637\n",
      "0.748993539930718\n",
      "0.9362419249133976\n",
      "0.9456043441625316\n",
      "0.9549667634116655\n",
      "0.9643291826607996\n",
      "0.9736916019099335\n",
      "0.9830540211590675\n",
      "0.9924164404082014\n",
      "80551.70564198494\n"
     ]
    }
   ],
   "source": [
    "#lets think about it for a second, 1.5 seconds on 1000x1600 matrix, in 10m dataset we have 70000x10000 matrix and\n",
    "#our complexity for constructing dev matrix is m*m*n so on 10m dataset we should work roughly 70*70*10 times longer\n",
    "#73500 seconds and it is 20 hours in approximation...little bit too long\n",
    "dev_mat_10m = np.zeros(shape=[train_10m_mat.shape[1],train_10m_mat.shape[1]])\n",
    "dev_mat_10m_user = np.zeros(shape=[train_10m_mat.shape[1],train_10m_mat.shape[1]])\n",
    "a = time()\n",
    "get_dev(train_10m_mat,dev_mat_10m,dev_mat_10m_user)\n",
    "print(time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.95820689201355\n",
      "MSE:  0.7350576486037158\n"
     ]
    }
   ],
   "source": [
    "a = time()\n",
    "res_slope_one_10m = predict_slope_one(train_10m_mat,test_10m[:,:2].astype(int),dev_mat_10m,dev_mat_10m_user,global_bias_10m)\n",
    "print(time()-a)\n",
    "print(\"MSE: \",np.mean((test_10m[:,2]-res_slope_one_10m)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD as SVDs\n",
    "from surprise import NormalPredictor\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import SlopeOne\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data2 = pd.read_csv(\"ml-10M100K/ratings.dat\",header = None,names = [\"user\",\"movie\",\"rating\",\"timestamp\"],sep = \"::\",encoding = \"utf8\",engine = \"python\")\n",
    "data2 = Dataset.load_from_df(data2[['user', 'movie', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MSE of algorithm SlopeOne on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MSE (testset)     0.7408  0.7406  0.7400  0.7392  0.7429  0.7407  0.0012  \n",
      "Fit time          214.46  228.06  222.59  229.80  224.76  223.93  5.36    \n",
      "Test time         687.88  697.55  734.83  708.24  708.57  707.41  15.70   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_mse': array([0.74080731, 0.74064798, 0.74002596, 0.73920154, 0.74286376]),\n",
       " 'fit_time': (214.4570689201355,\n",
       "  228.05578589439392,\n",
       "  222.58541989326477,\n",
       "  229.80457520484924,\n",
       "  224.7646279335022),\n",
       " 'test_time': (687.8847141265869,\n",
       "  697.5467760562897,\n",
       "  734.8269639015198,\n",
       "  708.2431640625,\n",
       "  708.5718669891357)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Surprise SlopeOne uses cython version of numpy and have some little bit optimized functions, but in general it works\n",
    "#the same way as my implementation\n",
    "algo = SlopeOne()\n",
    "cross_validate(algo, data2, measures=['MSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MSE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MSE (testset)     0.6364  0.6365  0.6377  0.6367  0.6358  0.6366  0.0006  \n",
      "Fit time          431.41  435.88  436.29  436.57  436.14  435.26  1.94    \n",
      "Test time         35.99   36.96   31.07   27.10   36.57   33.54   3.86    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_mse': array([0.63641685, 0.63645353, 0.63767043, 0.63674559, 0.63577264]),\n",
       " 'fit_time': (431.41488099098206,\n",
       "  435.88187885284424,\n",
       "  436.29262113571167,\n",
       "  436.5704970359802,\n",
       "  436.1357898712158),\n",
       " 'test_time': (35.991979122161865,\n",
       "  36.96283984184265,\n",
       "  31.070253133773804,\n",
       "  27.101269006729126,\n",
       "  36.572930097579956)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = SVDs()\n",
    "cross_validate(algo, data2, measures=['MSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funk SVD results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As could be seen,funk svd is doing the best out of all algorithms presented, but MSE when its nice to see some\n",
    "#general wellness of algorithm and easy is to compute, it does not provide us most important anwsers.\n",
    "#Recomender systems are meant to be used to recomend something to user based on their previous interactions\n",
    "#so lets construct another measure of algorithm and see how good funk svd is doing.\n",
    "@njit\n",
    "def recomendations_for_users(data,x,pu,qi,bu,bi,gb):\n",
    "    recomendations = []\n",
    "    for y in range(data.shape[1]):\n",
    "        if np.isnan(data[x][y]):\n",
    "            recomendations.append(((pu[x,:].dot(qi[y,:]))+bu[x]+bi[y]+gb,y))\n",
    "    recomendations.sort()\n",
    "    return recomendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firstly we will pick user, print out using movie.dat categories of his favourite movies,make some recomendations,\n",
    "#and see how good our recomendations fit in\n",
    "pu = factorized_funk_10m[0]\n",
    "qi = factorized_funk_10m[1]\n",
    "bu = bias_user\n",
    "bi = bias_item\n",
    "gb = global_bias_10m\n",
    "recomendations = recomendations_for_users(data_10m_mat,0,pu,qi,bu,bi,gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5.5009287536741835, 5941), (5.513155338175091, 235), (5.543432974113515, 3296)]\n"
     ]
    }
   ],
   "source": [
    "print(recomendations[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv(\"ml-10M100K/movies.dat\",header = None,names = [\"id\",\"title\",\"category\"],sep = \"::\",encoding = \"utf8\",engine = \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "#at first my if statement was >=4 but after some thought, every movie which is rated higher than user bias is supposed\n",
    "#to be movie liked by him/her\n",
    "@njit\n",
    "def find_best_user_movies(original_matrix,user):\n",
    "    movies_that_user_liked = []\n",
    "    user_bias = np.nanmean(original_matrix[user])\n",
    "    for y in range(original_matrix.shape[1]):\n",
    "        if not np.isnan(original_matrix[0][y]):\n",
    "            if original_matrix[0][y] >= user_bias:\n",
    "                movies_that_user_liked.append(y)\n",
    "    return movies_that_user_liked\n",
    "            \n",
    "movies_that_user_liked = find_best_user_movies(data_10m_mat,0)\n",
    "print(movies_that_user_liked)\n",
    "movies = []\n",
    "for movie in movies_that_user_liked:\n",
    "    movies.append(int(tables_10m[1][movie]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id             title        category\n",
      "120  122  Boomerang (1992)  Comedy|Romance\n",
      "      id            title               category\n",
      "183  185  Net, The (1995)  Action|Crime|Thriller\n",
      "      id                 title category\n",
      "228  231  Dumb & Dumber (1994)   Comedy\n",
      "      id            title                      category\n",
      "289  292  Outbreak (1995)  Action|Drama|Sci-Fi|Thriller\n",
      "      id            title                 category\n",
      "313  316  Stargate (1994)  Action|Adventure|Sci-Fi\n",
      "      id                          title                       category\n",
      "325  329  Star Trek: Generations (1994)  Action|Adventure|Drama|Sci-Fi\n",
      "      id                    title                 category\n",
      "351  355  Flintstones, The (1994)  Children|Comedy|Fantasy\n",
      "      id                title                  category\n",
      "352  356  Forrest Gump (1994)  Comedy|Drama|Romance|War\n",
      "      id                    title                    category\n",
      "358  362  Jungle Book, The (1994)  Adventure|Children|Romance\n",
      "      id                  title                                    category\n",
      "360  364  Lion King, The (1994)  Adventure|Animation|Children|Drama|Musical\n",
      "      id                                      title       category\n",
      "366  370  Naked Gun 33 1/3: The Final Insult (1994)  Action|Comedy\n",
      "      id         title                 category\n",
      "373  377  Speed (1994)  Action|Romance|Thriller\n",
      "      id                         title                      category\n",
      "416  420  Beverly Hills Cop III (1994)  Action|Comedy|Crime|Thriller\n",
      "      id                        title           category\n",
      "462  466  Hot Shots! Part Deux (1993)  Action|Comedy|War\n",
      "      id                 title                          category\n",
      "476  480  Jurassic Park (1993)  Action|Adventure|Sci-Fi|Thriller\n",
      "      id                             title category\n",
      "516  520  Robin Hood: Men in Tights (1993)   Comedy\n",
      "      id                        title              category\n",
      "535  539  Sleepless in Seattle (1993)  Comedy|Drama|Romance\n",
      "      id              title         category\n",
      "580  586  Home Alone (1990)  Children|Comedy\n",
      "      id           title                                     category\n",
      "582  588  Aladdin (1992)  Adventure|Animation|Children|Comedy|Musical\n",
      "      id                              title       category\n",
      "583  589  Terminator 2: Judgment Day (1991)  Action|Sci-Fi\n",
      "      id                                   title  \\\n",
      "588  594  Snow White and the Seven Dwarfs (1937)   \n",
      "\n",
      "                                     category  \n",
      "588  Animation|Children|Drama|Fantasy|Musical  \n",
      "      id                   title            category\n",
      "610  616  Aristocats, The (1970)  Animation|Children\n"
     ]
    }
   ],
   "source": [
    "movie_data.head()\n",
    "for a in movies:\n",
    "    print(movie_data.loc[movie_data['id']==a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                       title  \\\n",
      "1172  1197  Princess Bride, The (1987)   \n",
      "\n",
      "                                     category  \n",
      "1172  Action|Adventure|Comedy|Fantasy|Romance  \n",
      "        id                     title                  category\n",
      "6961  7075  Court Jester, The (1956)  Adventure|Comedy|Musical\n",
      "        id                     title category\n",
      "6867  6981  Word, The (Ordet) (1955)    Drama\n",
      "        id                      title                 category\n",
      "1242  1270  Back to the Future (1985)  Adventure|Comedy|Sci-Fi\n",
      "        id                  title                          category\n",
      "7878  8580  Into the Woods (1991)  Adventure|Comedy|Fantasy|Musical\n"
     ]
    }
   ],
   "source": [
    "best_ten = recomendations[-5:]\n",
    "for mv in best_ten:\n",
    "    print(movie_data.loc[movie_data['id']==int(tables_10m[1][mv[1]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#okay so now lets construct measure function for whole dataset,idea is as follows:\n",
    "#map each category to number, create matrix users x moviecategories, where cell describes how much user likes this\n",
    "#category, where equation for it will be:\n",
    "#if movie_rating is higher than 3 then 1+(movie_rating-bias)\n",
    "#if movie_rating is lower than 3 then -(1-(movie_rating-bias))\n",
    "#after that after we make recomendation we look up in our newly created matrix to check if categories of our movie\n",
    "#match favourite categories of user\n",
    "mapping = {}\n",
    "cnt = 0\n",
    "movie_data_mat = np.zeros(shape = [10681,20])\n",
    "for index,row in movie_data.iterrows():\n",
    "        categories = row[2].split('|')\n",
    "        for category in categories:\n",
    "            if not category in mapping:\n",
    "                mapping[category] = cnt\n",
    "                cnt+=1\n",
    "            if not row[0] in tables_10m[0]:\n",
    "                tables_10m[0][row[0]] = max(tables_10m[0].values())+1\n",
    "            x = tables_10m[0][row[0]]\n",
    "            y = mapping[category]\n",
    "            movie_data_mat[x,y] = 1\n",
    "print(len(mapping.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#well I tried drawing and imagining a lot of functions to properly describe user_likings but they either had a problem\n",
    "#of users having only 5 and then his liking would be 0, or user giving only 1 and these funcs would not take this\n",
    "#movies as not liked beacuse user_bias would be 1. So I decided to use linearly growing with distance from bias \n",
    "#multiplier, beacuse well it is simply and somehow captures all of this.\n",
    "@njit\n",
    "def create_user_like_mat(original_matrix,users_likings):\n",
    "    for user in range(original_matrix.shape[0]):\n",
    "        bias = np.nanmean(original_matrix[user])\n",
    "        for movie in range(original_matrix.shape[1]):\n",
    "            if not np.isnan(original_matrix[user,movie]):\n",
    "                if original_matrix[user,movie] >= 3:\n",
    "                    users_likings[user]+=(1+(original_matrix[user][movie]-bias)) * movie_data_mat[movie]\n",
    "                else:\n",
    "                    users_likings[user]-=(1-(original_matrix[user][movie]-bias)) * movie_data_mat[movie]\n",
    "\n",
    "users_likings = np.zeros(shape = [71567,20])\n",
    "create_user_like_mat(data_10m_mat,users_likings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan ... nan nan nan] 3.2\n",
      "[10.   0.   0.8  3.2  0.8  0.2  3.6 13.2 -2.2  2.6  0.8 -2.2  7.8  0.\n",
      "  0.   4.4  0.8  0.   2.8  0. ]\n"
     ]
    }
   ],
   "source": [
    "print(data_10m_mat[1],np.nanmean(data_10m_mat[1]))\n",
    "print(users_likings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7.18409091,  6.36233333, 11.32082185, ..., 37.72640892,\n",
       "       16.08540192, 10.20271746])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@njit\n",
    "def user_likings_eval_bias(original_matrix,users_likings_bias,users_likings,movie_data_mat):\n",
    "    empty_users = 0\n",
    "    for user in range(original_matrix.shape[0]):\n",
    "        cnt = 0\n",
    "        for movie in range(original_matrix.shape[1]):\n",
    "            if not np.isnan(original_matrix[user,movie]):\n",
    "                users_likings_bias[user] += users_likings[user].dot(movie_data_mat[movie])/(np.sum(movie_data_mat[movie]))\n",
    "                cnt+=1\n",
    "        if cnt==0:\n",
    "            empty_users +=1\n",
    "            users_likings_bias[user] = np.nan\n",
    "        else:\n",
    "            users_likings_bias[user] /= cnt\n",
    "    return empty_users\n",
    "\n",
    "users_likings_bias = np.zeros(shape = [71567])\n",
    "empty_users = user_likings_eval_bias(data_10m_mat,users_likings_bias,users_likings,movie_data_mat)\n",
    "print(empty_users)\n",
    "users_likings_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.013972920480109547\n",
      "0.027945840960219095\n",
      "0.04191876144032864\n",
      "0.05589168192043819\n",
      "0.06986460240054773\n",
      "0.08383752288065728\n",
      "0.09781044336076683\n",
      "0.11178336384087638\n",
      "0.12575628432098593\n",
      "0.13972920480109546\n",
      "0.15370212528120503\n",
      "0.16767504576131456\n",
      "0.18164796624142412\n",
      "0.19562088672153366\n",
      "0.20959380720164322\n",
      "0.22356672768175276\n",
      "0.23753964816186232\n",
      "0.25151256864197186\n",
      "0.2654854891220814\n",
      "0.2794584096021909\n",
      "0.2934313300823005\n",
      "0.30740425056241005\n",
      "0.3213771710425196\n",
      "0.3353500915226291\n",
      "0.3493230120027387\n",
      "0.36329593248284825\n",
      "0.3772688529629578\n",
      "0.3912417734430673\n",
      "0.4052146939231769\n",
      "0.41918761440328645\n",
      "0.43316053488339595\n",
      "0.4471334553635055\n",
      "0.4611063758436151\n",
      "0.47507929632372464\n",
      "0.48905221680383415\n",
      "0.5030251372839437\n",
      "0.5169980577640533\n",
      "0.5309709782441628\n",
      "0.5449438987242724\n",
      "0.5589168192043819\n",
      "0.5728897396844914\n",
      "0.586862660164601\n",
      "0.6008355806447105\n",
      "0.6148085011248201\n",
      "0.6287814216049297\n",
      "0.6427543420850392\n",
      "0.6567272625651488\n",
      "0.6707001830452582\n",
      "0.6846731035253678\n",
      "0.6986460240054774\n",
      "0.7126189444855869\n",
      "0.7265918649656965\n",
      "0.7405647854458061\n",
      "0.7545377059259156\n",
      "0.7685106264060251\n",
      "0.7824835468861346\n",
      "0.7964564673662442\n",
      "0.8104293878463538\n",
      "0.8244023083264633\n",
      "0.8383752288065729\n",
      "0.8523481492866825\n",
      "0.8663210697667919\n",
      "0.8802939902469015\n",
      "0.894266910727011\n",
      "0.9082398312071206\n",
      "0.9222127516872302\n",
      "0.9361856721673397\n",
      "0.9501585926474493\n",
      "0.9641315131275587\n",
      "0.9781044336076683\n",
      "0.9920773540877779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6895523294170799"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@njit\n",
    "def find_category_deviation(original_matrix,users_likings,users_likings_bias,movie_data_mat,tables,pu,qi,bu,bi,gb):\n",
    "    general_score = 0\n",
    "    cnt = 0\n",
    "    for user in range(original_matrix.shape[0]):\n",
    "        if user%1000==0:\n",
    "            print(user/original_matrix.shape[0])\n",
    "        if np.isnan(np.nanmean(original_matrix[user])):\n",
    "            continue\n",
    "        recomend = recomendations_for_users(original_matrix,user,pu,qi,bu,bi,gb)\n",
    "        recomendations = recomend[:-10]\n",
    "        for r in range(len(recomendations)):\n",
    "            categories = movie_data_mat[recomendations[r][1]]\n",
    "            indexes = (categories == 1)\n",
    "            row = users_likings[user]\n",
    "            res = np.sum(row[indexes])\n",
    "            cnt+=1\n",
    "            if res >= users_likings_bias[user]:\n",
    "                general_score+=1\n",
    "    return general_score/cnt\n",
    "\n",
    "find_category_deviation(data_10m_mat,users_likings,users_likings_bias,movie_data_mat,tables_10m,pu,qi,bu,bi,gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future predicting using svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10m_future = pd.read_csv(\"ml-10M100K/ratings.dat\",header = None,names = [\"user\",\"movie\",\"rating\",\"timestamp\"],sep = \"::\",encoding = \"utf8\",engine = \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10m_future = data_10m_future.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = hash_movies(data_10m_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10m_future = data_10m_future.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10m_future_all = data_10m_future[data_10m_future[:,3].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     36955,         71,          3,  789652009],\n",
       "       [     36955,        102,          5,  789652009],\n",
       "       [     36955,        387,          3,  789652009],\n",
       "       ...,\n",
       "       [     63100,       1358,          2, 1231131142],\n",
       "       [     62510,       2228,          2, 1231131303],\n",
       "       [     62510,       4538,          3, 1231131736]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_10m_future_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10m_future,test_10m_future = np.split(data_10m_future_all,[int(0.8*data_10m_future_all.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10m_future_mat = construct_Matrix(train_10m_future,71567,10681)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122.91508722305298\n"
     ]
    }
   ],
   "source": [
    "global_bias_future = np.nanmean(train_10m_future_mat)\n",
    "bias_user_future = np.zeros(train_10m_future_mat.shape[0])\n",
    "bias_item_future = np.zeros(train_10m_future_mat.shape[1])\n",
    "non_nan_future = train_10m_future[:,:2]\n",
    "non_nan_future[:,0] -= 1\n",
    "non_nan_future = non_nan_future.astype(int)\n",
    "size = 100\n",
    "users_future = np.random.normal(scale = 0.1,size = [train_10m_future_mat.shape[0],size])\n",
    "items_future = np.random.normal(scale = 0.1,size = [train_10m_future_mat.shape[1],size])\n",
    "a = time()\n",
    "factorized_funk_10m_future = matrix_factorize_funk(20,train_10m_future_mat,non_nan_future,global_bias_future,bias_user_future,bias_item_future,0.005,0.02,size,users_future,items_future)\n",
    "print(time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.70443296432495\n",
      "MSE:  1.2245112498381525\n"
     ]
    }
   ],
   "source": [
    "a = time()\n",
    "res_10m_future = np.dot(factorized_funk_10m_future[0],factorized_funk_10m_future[1].T)\n",
    "tes_future = test_10m_future[:,:2].astype(int)\n",
    "result_10m_future = []\n",
    "for user_x,item_y in tes_future:\n",
    "    result_10m_future.append(global_bias_future+bias_user_future[user_x]+bias_item_future[item_y]+res_10m_future[user_x,item_y])\n",
    "result_10m_future = np.array(result_10m_future)\n",
    "print(time()-a)\n",
    "print(\"MSE: \",np.mean((test_10m_future[:,2]-result_10m_future)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import SlopeOne\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "train_10m_future_df = pd.DataFrame({'user':train_10m_future[:,0],'movie':train_10m_future[:,1],'rating':train_10m_future[:,2],'timestamp':train_10m_future[:,3]})\n",
    "data_train_future = Dataset.load_from_df(train_10m_future_df[['user', 'movie', 'rating']], reader)\n",
    "trainset = data_train_future.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404.9072380065918\n"
     ]
    }
   ],
   "source": [
    "algo_future = SVD()\n",
    "a = time()\n",
    "algo_future.fit(trainset)\n",
    "print(time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  1.1691525044047777\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for row in test_10m_future:\n",
    "    preds.append(row[2]-algo_future.predict(row[0],row[1],row[2])[3])\n",
    "preds = np.array(preds)\n",
    "print(\"MSE: \", np.mean(preds**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
